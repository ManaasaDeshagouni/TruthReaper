# TruthReaper â€“ Real-Time Deception Detection from Speech and Text

**Authors:**  
- Manasa Deshagouni  
- Dheeraj Kumar Alla  
**Institution:** San Jose State University  
**Course:** CS286 â€“ Advanced Topics in Computer Science  
**Project Type:** Final Research Submission

---

## ğŸ“Œ Overview

TruthReaper is a **dual-track deception detection system** designed to classify spoken statements as **truthful** or **deceptive**. The system leverages both acoustic and semantic signals from speech through two complementary machine learning pipelines:

---

### ğŸ” Track 1 â€“ Hybrid LSTM (Manasa)

- Extracts **sequential time-series features** from `.wav` audio:
  - `pitch_seq`, `energy_seq`, `hesitation_seq`, `disfluency_seq`
- Computes 12 **summary features** like:
  - pause duration, disfluency rate, pitch variance, etc.
- Combines both using a **Bidirectional LSTM**
- Trains using **weighted cross-entropy**
- Augments training data using a **conditional GAN**
- Supports **real-time voice-based prediction** via microphone + Whisper



---

## ğŸ“Š Evaluation

- **Dataset**: Real-Life Trial (RLT) + self-recorded `.wav` clips
- **Cross-validation**: 5-Fold
- **Track 1** â€“ LSTM:
  - F1 Score: **0.798**, Accuracy: 80.4%

---

## ğŸ—‚ Folder Structure
TruthReaper/
â”œâ”€â”€ analysis/                       # Stores evaluation plots and prediction reports
â”œâ”€â”€ clips/                          # Place your raw .wav data here (truthful, deception folders)
â”œâ”€â”€ env/ / venv/                    # Virtual environment folders (optional)
â”œâ”€â”€ models/                         # Whisper + saved model weights
â”œâ”€â”€ recordings/                     # Real-time recorded clips (auto-created)
â”œâ”€â”€ *.py                            # All training, inference, and feature scripts
â”œâ”€â”€ *.json                          # Processed datasets and synthetic data
â”œâ”€â”€ README.md                       # Project guide (this file)
â”œâ”€â”€ requirements.txt                # Dependency list

---

## ğŸ“¦ File Descriptions

### ğŸ”„ Feature Extraction
- `batch_feature_extractor.py` â€“ Main extractor for all audio features (sequential + summary)
- `pause_anlyzer.py` â€“ Identifies hesitation/pause segments in speech
- `disfluency_extractor.py` â€“ Uses Whisper transcription to find fillers, stutters, repetitions
- `emotion_analyzer.py` â€“ Extracts average energy and pitch variance for emotion cues
- `audio_processor.py` / `feature_extractor.py` â€“ Older feature modules (optional)

---

### ğŸ¤– Modeling & Training
- `sequence_lstm_trainer.py` â€“ Base LSTM model trainer
- `k_fold_trainer.py` â€“ Performs 5-fold cross-validation and saves results
- `truthreaper_hybrid_lstm.pt` â€“ Final trained BiLSTM model

---

### ğŸ§ª Synthetic Data
- `gan_trainer.py` â€“ Trains a GAN for synthetic time-series generation
- `gan_generator.py` â€“ Internal generator class
- `generator_truth.pt`, `generator_lie.pt` â€“ Trained GAN models
- `synthetic_full_truth.json`, `synthetic_full_lie.json` â€“ GAN-generated sample datasets
- `merge_datasets.py` â€“ Combines real and synthetic samples into one JSON

---

### ğŸ¤ Inference & Real-Time
- `truth_reaper_transcriber.py` â€“ Record audio + transcribe + predict (full pipeline)
- `truth_recorder.py`, `lie_recorder.py` â€“ Save mic input directly to respective folders
- `test-input-01.txt` â€“ Test transcripts for evaluation
- `video_to_audio_converter.py` â€“ Extracts audio from video files for labeling

---

## ğŸ“¦ Installation

Create a virtual environment (optional) and install dependencies:

```bash
pip install -r requirements.txt

ğŸ“‚ Dataset Setup

ğŸ”º The dataset is not provided in this repo. You must download it manually.

	1.	Download from:
https://archive.ics.uci.edu/ml/datasets/Real+Life+Trial+Dataset
	2.	Place your .wav files in this structure:
  /clips/
â”œâ”€â”€ truthful/
â”‚   â”œâ”€â”€ trial_truth_001.wav
â”‚   â””â”€â”€ ...
â””â”€â”€ deception/
    â”œâ”€â”€ trial_lie_001.wav
    â””â”€â”€ ...

ğŸš€ How to Run the Project

âœ… 1. Feature Extraction:
python3 batch_feature_extractor.py --limit 100

âœ”ï¸ Generates sequence_dataset.json with audio features

âœ… 2. (Optional) Generate Synthetic Data:
python3 gan_trainer.py --label truth --epochs 5000
python3 gan_trainer.py --label lie --epochs 5000

then merge them:
python3 merge_datasets.py
âœ”ï¸ Creates sequence_dataset_combined.json

âœ… 3. Train the LSTM Model (Track 1):
python3 k_fold_trainer.py

âœ”ï¸ Performs 5-fold CV
âœ”ï¸ Saves truthreaper_hybrid_lstm.pt
âœ”ï¸ Saves evaluation plot as kfold_metrics.png

âœ… 4. Real-Time Prediction (Microphone)
python3 truth_reaper_transcriber.py

	â€¢	Records your voice
	â€¢	Predicts â€œtruthâ€ or â€œlieâ€
	â€¢	Logs to /analysis/reports/

â— Notes
	â€¢	Whisper ASR is downloaded automatically via Huggingface (whisper-base)
	â€¢	Project assumes single-speaker English voice recordings

ğŸ“š References
	â€¢	Whisper: https://github.com/openai/whisper
	â€¢	RLT Dataset: https://archive.ics.uci.edu/ml/datasets/Real+Life+Trial+Dataset
	â€¢	Librosa: https://librosa.org

ğŸ™ Acknowledgments

We thank Prof. Amith Kamath Belman for valuable feedback and research guidance, and acknowledge the use of OpenAI Whisper and Huggingface Transformers in this project.
